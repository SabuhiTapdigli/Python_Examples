{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "07766905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pygad\n",
    "import tensorflow\n",
    "import tensorflow.keras\n",
    "import pygad.kerasga\n",
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cebdc55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alaska (Logs_Result)\n",
    "df1 = pd.read_csv('Alaska (Logs_Result).txt', delimiter = \"\\t\")\n",
    "# Alaska_TOC-core\n",
    "df2 = pd.read_csv('Alaska_TOC-core.txt', delimiter = \"\\t\")\n",
    "# Synthetic Well\n",
    "df3 = pd.read_csv('Synthetic Well.txt', delimiter = \"\\t\")\n",
    "# TOC Vker Synth\n",
    "df4 = pd.read_csv('TOC_Vker_SynthLog.txt', delimiter = \"\\t\")\n",
    "# hungarty\n",
    "df5 = pd.read_csv('Datafull_Hu.txt', delimiter = \"\\t\")\n",
    "# Norway\n",
    "df6 = pd.read_csv('Norway data.txt', delimiter = \"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2b26a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1[['ILD','NPHI','GR','RHOB','DT']].to_numpy()\n",
    "Y = df1['TOCi'].to_numpy()\n",
    "# sc = StandardScaler()\n",
    "minmax = MinMaxScaler()\n",
    "X_std =minmax.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, Y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bb9eddfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14376443, 0.46206891, 0.09630731, 0.84217214, 0.3304062 ],\n",
       "       [0.14167599, 0.4660459 , 0.09839231, 0.84954701, 0.32767117],\n",
       "       [0.14165396, 0.4731325 , 0.1013698 , 0.85880013, 0.32481404],\n",
       "       ...,\n",
       "       [0.98993485, 0.04165655, 0.13432213, 0.87926748, 0.04630502],\n",
       "       [0.99562481, 0.04545573, 0.13299112, 0.88628328, 0.04504007],\n",
       "       [1.        , 0.0507555 , 0.13209864, 0.89247045, 0.04421468]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1b865323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation = 1\n",
      "Fitness    = 0.9074272670872338\n",
      "Generation = 2\n",
      "Fitness    = 0.9149945618319042\n",
      "Generation = 3\n",
      "Fitness    = 0.9234799021205242\n",
      "Generation = 4\n",
      "Fitness    = 0.9234799021205242\n",
      "Generation = 5\n",
      "Fitness    = 0.968887981164624\n",
      "Generation = 6\n",
      "Fitness    = 0.968887981164624\n",
      "Generation = 7\n",
      "Fitness    = 0.9710518246350854\n",
      "Generation = 8\n",
      "Fitness    = 0.9784732721563243\n",
      "Generation = 9\n",
      "Fitness    = 0.9973726244068358\n",
      "Generation = 10\n",
      "Fitness    = 1.0040111955386062\n",
      "Generation = 11\n",
      "Fitness    = 1.0061992176365127\n",
      "Generation = 12\n",
      "Fitness    = 1.0093658482481564\n",
      "Generation = 13\n",
      "Fitness    = 1.0111401677878542\n",
      "Generation = 14\n",
      "Fitness    = 1.0111401677878542\n",
      "Generation = 15\n",
      "Fitness    = 1.0111401677878542\n",
      "Generation = 16\n",
      "Fitness    = 1.0111401677878542\n",
      "Generation = 17\n",
      "Fitness    = 1.0121519308879363\n",
      "Generation = 18\n",
      "Fitness    = 1.0132548131855659\n",
      "Generation = 19\n",
      "Fitness    = 1.0137284406429112\n",
      "Generation = 20\n",
      "Fitness    = 1.0145660241517709\n",
      "Generation = 21\n",
      "Fitness    = 1.0145660241517709\n",
      "Generation = 22\n",
      "Fitness    = 1.0145673739348882\n",
      "Generation = 23\n",
      "Fitness    = 1.0145790313021685\n",
      "Generation = 24\n",
      "Fitness    = 1.0145790313021685\n",
      "Generation = 25\n",
      "Fitness    = 1.0145790313021685\n",
      "Generation = 26\n",
      "Fitness    = 1.0145983585802603\n",
      "Generation = 27\n",
      "Fitness    = 1.0147437969543103\n",
      "Generation = 28\n",
      "Fitness    = 1.0147437969543103\n",
      "Generation = 29\n",
      "Fitness    = 1.0147437969543103\n",
      "Generation = 30\n",
      "Fitness    = 1.0149040729104766\n",
      "Generation = 31\n",
      "Fitness    = 1.0149040729104766\n",
      "Generation = 32\n",
      "Fitness    = 1.0149040729104766\n",
      "Generation = 33\n",
      "Fitness    = 1.0150701110379057\n",
      "Generation = 34\n",
      "Fitness    = 1.0150701110379057\n",
      "Generation = 35\n",
      "Fitness    = 1.0150701110379057\n",
      "Generation = 36\n",
      "Fitness    = 1.0150701110379057\n",
      "Generation = 37\n",
      "Fitness    = 1.0150701110379057\n",
      "Generation = 38\n",
      "Fitness    = 1.0150701110379057\n",
      "Generation = 39\n",
      "Fitness    = 1.0150701110379057\n",
      "Generation = 40\n",
      "Fitness    = 1.0150701110379057\n",
      "Generation = 41\n",
      "Fitness    = 1.0151037673977361\n",
      "Generation = 42\n",
      "Fitness    = 1.0151037673977361\n",
      "Generation = 43\n",
      "Fitness    = 1.0151037673977361\n",
      "Generation = 44\n",
      "Fitness    = 1.0151037673977361\n",
      "Generation = 45\n",
      "Fitness    = 1.0151853380538307\n",
      "Generation = 46\n",
      "Fitness    = 1.0151853380538307\n",
      "Generation = 47\n",
      "Fitness    = 1.0151853380538307\n",
      "Generation = 48\n",
      "Fitness    = 1.0151853380538307\n",
      "Generation = 49\n",
      "Fitness    = 1.0151853380538307\n",
      "Generation = 50\n",
      "Fitness    = 1.0151853380538307\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness value of the best solution = 1.0151853380538307\n",
      "Index of the best solution : 0\n",
      "Predictions : \n",
      " [[2.7914743]\n",
      " [2.644612 ]\n",
      " [2.7826006]\n",
      " [2.8257356]\n",
      " [2.7945967]\n",
      " [2.7988737]\n",
      " [2.7256153]\n",
      " [2.8595715]\n",
      " [2.8038783]\n",
      " [2.7838726]\n",
      " [2.711159 ]\n",
      " [2.7642546]\n",
      " [2.7730067]\n",
      " [2.9437912]\n",
      " [2.7541656]\n",
      " [2.7591376]\n",
      " [2.7607622]\n",
      " [2.7106085]\n",
      " [2.7600489]\n",
      " [2.6960459]\n",
      " [2.8015518]\n",
      " [2.7292023]\n",
      " [2.820434 ]\n",
      " [2.7751985]\n",
      " [2.7038364]\n",
      " [2.7761867]\n",
      " [2.706741 ]\n",
      " [2.784268 ]\n",
      " [2.8313127]\n",
      " [2.6790771]\n",
      " [2.6816125]\n",
      " [2.6792939]\n",
      " [2.678838 ]\n",
      " [2.7861867]\n",
      " [2.7163217]\n",
      " [2.6815941]\n",
      " [2.761069 ]\n",
      " [2.7106965]\n",
      " [2.7795267]\n",
      " [2.9079933]\n",
      " [2.7925737]\n",
      " [2.7590327]\n",
      " [2.6969848]\n",
      " [2.679963 ]\n",
      " [2.8025837]\n",
      " [2.7244153]\n",
      " [2.7189326]\n",
      " [2.7449245]\n",
      " [2.657098 ]\n",
      " [2.7941961]\n",
      " [2.8372555]\n",
      " [2.780717 ]\n",
      " [2.7887022]\n",
      " [2.6961248]\n",
      " [2.7653804]\n",
      " [2.8804753]\n",
      " [2.8216338]\n",
      " [2.9749956]\n",
      " [2.7631555]\n",
      " [2.6707668]\n",
      " [2.7935395]\n",
      " [2.972618 ]\n",
      " [2.7651732]\n",
      " [2.6886566]\n",
      " [2.7151656]\n",
      " [2.7074842]\n",
      " [2.8229208]\n",
      " [2.9108357]\n",
      " [2.7924623]\n",
      " [2.675975 ]\n",
      " [2.7882307]\n",
      " [2.911635 ]\n",
      " [2.675549 ]\n",
      " [2.7575796]\n",
      " [2.7523322]\n",
      " [2.7434855]\n",
      " [2.7821484]\n",
      " [2.774318 ]\n",
      " [2.6824217]\n",
      " [2.8012617]\n",
      " [2.7041993]\n",
      " [2.659967 ]\n",
      " [2.8146756]\n",
      " [2.7429209]\n",
      " [2.7068489]\n",
      " [2.791019 ]\n",
      " [2.691716 ]\n",
      " [2.85358  ]\n",
      " [2.6904268]\n",
      " [2.682868 ]\n",
      " [2.7201438]\n",
      " [2.7647545]\n",
      " [2.7572083]\n",
      " [2.9149342]\n",
      " [2.6499734]\n",
      " [2.744841 ]\n",
      " [2.7713523]\n",
      " [2.744254 ]\n",
      " [2.653667 ]\n",
      " [2.791356 ]\n",
      " [2.7658062]\n",
      " [2.7925148]\n",
      " [2.844212 ]\n",
      " [2.793756 ]\n",
      " [2.7979503]\n",
      " [2.701046 ]\n",
      " [2.7854834]\n",
      " [2.7957237]\n",
      " [2.8319235]\n",
      " [2.8080006]\n",
      " [2.7478437]\n",
      " [2.774949 ]\n",
      " [2.775969 ]\n",
      " [2.7861404]\n",
      " [2.7666192]\n",
      " [2.6563962]\n",
      " [2.9048643]\n",
      " [2.7103791]\n",
      " [2.7687044]\n",
      " [2.7814128]\n",
      " [2.7285945]\n",
      " [2.6522589]\n",
      " [2.7071896]\n",
      " [2.7597408]\n",
      " [2.7482464]\n",
      " [2.857149 ]\n",
      " [2.8525186]\n",
      " [2.8246112]\n",
      " [2.7832956]\n",
      " [2.7532144]\n",
      " [2.7352257]\n",
      " [2.6897578]\n",
      " [2.7929883]\n",
      " [2.6922863]\n",
      " [2.659024 ]\n",
      " [2.7825232]\n",
      " [2.7606943]\n",
      " [2.845656 ]\n",
      " [2.6507928]\n",
      " [2.703959 ]\n",
      " [2.8927376]\n",
      " [2.976529 ]\n",
      " [2.7860117]\n",
      " [2.7918139]\n",
      " [2.6896062]\n",
      " [2.7613807]\n",
      " [2.6720023]\n",
      " [2.9477112]\n",
      " [2.759747 ]\n",
      " [2.8147001]\n",
      " [2.7754827]\n",
      " [2.7040968]\n",
      " [2.7933645]\n",
      " [2.7020195]\n",
      " [2.6988816]\n",
      " [2.7443228]\n",
      " [2.67513  ]\n",
      " [2.803562 ]\n",
      " [2.7531428]\n",
      " [2.8112793]\n",
      " [2.7098103]\n",
      " [2.6768498]\n",
      " [2.7571201]\n",
      " [2.8033414]\n",
      " [2.7092965]\n",
      " [2.7017398]\n",
      " [2.776894 ]\n",
      " [2.7970533]\n",
      " [2.707491 ]\n",
      " [2.7918584]\n",
      " [2.7510014]\n",
      " [2.7490454]\n",
      " [2.7531543]\n",
      " [2.694921 ]\n",
      " [2.7544959]\n",
      " [2.850894 ]\n",
      " [2.7444236]\n",
      " [2.6906247]\n",
      " [2.7094533]\n",
      " [2.7003212]\n",
      " [2.7310743]\n",
      " [2.6788042]\n",
      " [2.730628 ]\n",
      " [2.7541873]\n",
      " [2.820881 ]\n",
      " [2.6743472]\n",
      " [2.7122378]\n",
      " [2.7463422]\n",
      " [2.6949177]\n",
      " [2.779695 ]\n",
      " [2.7041364]\n",
      " [2.8316283]\n",
      " [2.7901006]\n",
      " [2.7223535]\n",
      " [2.7710216]\n",
      " [2.7077594]\n",
      " [2.761406 ]\n",
      " [2.7804813]\n",
      " [2.7490108]\n",
      " [2.704238 ]\n",
      " [2.7714705]\n",
      " [2.6919096]\n",
      " [2.6536932]\n",
      " [2.7842782]\n",
      " [2.6642857]\n",
      " [2.75533  ]\n",
      " [2.7821586]\n",
      " [2.7079244]\n",
      " [2.6943827]\n",
      " [2.8431468]\n",
      " [2.818866 ]\n",
      " [2.6816876]\n",
      " [2.875671 ]\n",
      " [2.6770623]\n",
      " [2.9401736]\n",
      " [2.7805738]\n",
      " [2.6544533]\n",
      " [2.689629 ]\n",
      " [2.7414203]\n",
      " [2.7868624]\n",
      " [2.89891  ]\n",
      " [2.6764917]\n",
      " [2.8668668]\n",
      " [2.8479743]\n",
      " [2.7388854]\n",
      " [2.8096976]\n",
      " [2.8171277]\n",
      " [2.718216 ]\n",
      " [2.753921 ]\n",
      " [2.6531172]\n",
      " [2.7705488]\n",
      " [2.7057776]\n",
      " [2.6788077]\n",
      " [2.8558416]\n",
      " [2.7352912]\n",
      " [2.7119794]\n",
      " [2.976263 ]\n",
      " [2.7906594]\n",
      " [2.675543 ]\n",
      " [2.8087983]\n",
      " [2.772596 ]\n",
      " [2.6871521]\n",
      " [2.704181 ]\n",
      " [2.7909763]\n",
      " [2.8177173]\n",
      " [2.7939541]\n",
      " [2.7863212]\n",
      " [2.677678 ]\n",
      " [2.7661138]\n",
      " [2.7220411]\n",
      " [2.6887088]\n",
      " [2.647651 ]\n",
      " [2.7205245]\n",
      " [2.6833608]\n",
      " [2.7620215]\n",
      " [2.6732578]\n",
      " [2.6605098]\n",
      " [2.8264794]\n",
      " [2.7198148]\n",
      " [2.7655926]\n",
      " [2.7595844]\n",
      " [2.8697028]\n",
      " [2.7565782]\n",
      " [2.7273102]\n",
      " [2.7351003]\n",
      " [2.7695026]\n",
      " [2.8233922]\n",
      " [2.6830592]\n",
      " [2.8031507]\n",
      " [2.7602837]\n",
      " [2.7771883]\n",
      " [2.7991037]\n",
      " [2.8836994]\n",
      " [2.7235088]\n",
      " [2.7686484]\n",
      " [2.724867 ]\n",
      " [2.7625232]\n",
      " [2.7801504]\n",
      " [2.6874413]\n",
      " [2.6833649]\n",
      " [2.7511086]\n",
      " [2.7582116]\n",
      " [2.714625 ]\n",
      " [2.7618577]\n",
      " [2.711267 ]\n",
      " [2.8809   ]\n",
      " [2.7338574]\n",
      " [2.7079825]\n",
      " [2.7695737]\n",
      " [2.6843846]\n",
      " [2.7133775]\n",
      " [2.7895927]\n",
      " [2.8166249]\n",
      " [2.7309775]\n",
      " [2.7720156]\n",
      " [2.9052877]\n",
      " [2.797621 ]\n",
      " [2.7900205]\n",
      " [2.765106 ]\n",
      " [2.802493 ]\n",
      " [2.656837 ]\n",
      " [2.6934881]\n",
      " [2.7988338]\n",
      " [2.6569016]\n",
      " [2.7802134]\n",
      " [2.815003 ]\n",
      " [2.6765716]\n",
      " [2.6660416]\n",
      " [2.7968674]\n",
      " [2.7755466]\n",
      " [2.8974066]\n",
      " [2.6694353]\n",
      " [2.7002869]\n",
      " [2.7967277]\n",
      " [2.7447257]\n",
      " [2.9622755]\n",
      " [2.7484608]\n",
      " [2.7888227]\n",
      " [2.8685927]\n",
      " [2.8022795]\n",
      " [2.6817265]\n",
      " [2.7916102]\n",
      " [2.7198503]\n",
      " [2.7977405]\n",
      " [2.7110093]\n",
      " [2.7199028]\n",
      " [2.774387 ]\n",
      " [2.7881508]\n",
      " [2.8258953]\n",
      " [2.809825 ]\n",
      " [2.7142315]\n",
      " [2.864056 ]\n",
      " [2.7935674]\n",
      " [2.7562985]\n",
      " [2.7046053]\n",
      " [2.74879  ]\n",
      " [2.755979 ]\n",
      " [2.6969314]\n",
      " [2.6528234]\n",
      " [2.7086437]\n",
      " [2.7561069]\n",
      " [2.7648637]\n",
      " [2.7772818]\n",
      " [2.7733824]\n",
      " [2.7750628]\n",
      " [2.8053064]\n",
      " [2.829502 ]\n",
      " [2.7937994]\n",
      " [2.7523098]\n",
      " [2.679989 ]\n",
      " [2.68391  ]\n",
      " [2.7889237]\n",
      " [2.7286513]\n",
      " [2.7654486]\n",
      " [2.6469665]\n",
      " [2.7954502]\n",
      " [2.7918687]\n",
      " [2.8141122]\n",
      " [2.9340076]\n",
      " [2.7821307]\n",
      " [2.6656897]\n",
      " [2.7755451]\n",
      " [2.7675874]\n",
      " [2.672805 ]\n",
      " [2.7673168]\n",
      " [2.7859037]\n",
      " [2.7805738]\n",
      " [2.7132077]\n",
      " [2.8046036]\n",
      " [2.7668974]\n",
      " [2.840783 ]\n",
      " [2.698863 ]\n",
      " [2.6814973]\n",
      " [2.6669648]\n",
      " [2.7672594]\n",
      " [2.7627194]\n",
      " [2.7203302]\n",
      " [2.7733886]\n",
      " [2.7909913]\n",
      " [2.8582335]\n",
      " [2.6458275]\n",
      " [2.790906 ]\n",
      " [2.6521034]\n",
      " [2.72416  ]\n",
      " [2.7697768]\n",
      " [2.6728768]\n",
      " [2.6577399]\n",
      " [2.8285909]\n",
      " [2.7795424]\n",
      " [2.708407 ]\n",
      " [2.748036 ]\n",
      " [2.7725368]\n",
      " [2.663906 ]\n",
      " [2.7912734]\n",
      " [2.784521 ]\n",
      " [2.7184494]\n",
      " [2.7411122]\n",
      " [2.8102384]\n",
      " [2.7725673]\n",
      " [2.8021984]\n",
      " [2.789057 ]\n",
      " [2.9405212]\n",
      " [2.7801466]\n",
      " [2.6591523]\n",
      " [2.7108188]\n",
      " [2.7187767]\n",
      " [2.6622982]\n",
      " [2.8239317]\n",
      " [2.7118943]\n",
      " [2.7513325]\n",
      " [2.6888852]\n",
      " [2.7985477]\n",
      " [2.7342856]\n",
      " [2.7060099]\n",
      " [2.7825148]\n",
      " [2.761202 ]\n",
      " [2.6687102]\n",
      " [2.792537 ]\n",
      " [2.815412 ]\n",
      " [2.6568432]]\n",
      "Absolute Error :  0.9850418\n"
     ]
    }
   ],
   "source": [
    "def fitness_func(solution, sol_idx):\n",
    "    global data_inputs, data_outputs, keras_ga, model\n",
    "\n",
    "    predictions = pygad.kerasga.predict(model=model,\n",
    "                                        solution=solution,\n",
    "                                        data=data_inputs)\n",
    "\n",
    "    mae = tensorflow.keras.losses.MeanAbsoluteError()\n",
    "    abs_error = mae(data_outputs, predictions).numpy() + 0.00000001\n",
    "    solution_fitness = 1.0/abs_error\n",
    "\n",
    "    return solution_fitness\n",
    "\n",
    "def callback_generation(ga_instance):\n",
    "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
    "    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n",
    "\n",
    "input_layer  = tensorflow.keras.layers.Input(5)\n",
    "dense_layer1 = tensorflow.keras.layers.Dense(5, activation=\"relu\")(input_layer)\n",
    "output_layer = tensorflow.keras.layers.Dense(1, activation=\"linear\")(dense_layer1)\n",
    "\n",
    "model = tensorflow.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "keras_ga = pygad.kerasga.KerasGA(model=model,\n",
    "                                 num_solutions=10)\n",
    "\n",
    "# # Data inputs\n",
    "# data_inputs = numpy.array([[0.02, 0.1, 0.15],\n",
    "#                            [0.7, 0.6, 0.8],\n",
    "#                            [1.5, 1.2, 1.7],\n",
    "#                            [3.2, 2.9, 3.1]])\n",
    "data_inputs = X_train\n",
    "data_outputs = y_train\n",
    "test_inputs = X_test\n",
    "# # Data outputs\n",
    "# data_outputs = numpy.array([[0.1],\n",
    "#                             [0.6],\n",
    "#                             [1.3],\n",
    "#                             [2.5]])\n",
    "\n",
    "# Prepare the PyGAD parameters. Check the documentation for more information: https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#pygad-ga-class\n",
    "num_generations = 50 # Number of generations.\n",
    "num_parents_mating = 2 # Number of solutions to be selected as parents in the mating pool.\n",
    "initial_population = keras_ga.population_weights # Initial population of network weights\n",
    "\n",
    "ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                       num_parents_mating=num_parents_mating,\n",
    "                       initial_population=initial_population,\n",
    "                       fitness_func=fitness_func,\n",
    "                       on_generation=callback_generation)\n",
    "\n",
    "ga_instance.run()\n",
    "\n",
    "# After the generations complete, some plots are showed that summarize how the outputs/fitness values evolve over generations.\n",
    "ga_instance.plot_fitness(title=\"PyGAD & Keras - Iteration vs. Fitness\", linewidth=4)\n",
    "\n",
    "# Returning the details of the best solution.\n",
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\n",
    "print(\"Index of the best solution : {solution_idx}\".format(solution_idx=solution_idx))\n",
    "\n",
    "# Make prediction based on the best solution.\n",
    "predictions = pygad.kerasga.predict(model=model,\n",
    "                                    solution=solution,\n",
    "                                    data=data_inputs)\n",
    "print(\"Predictions : \\n\", predictions)\n",
    "test_pred = pygad.kerasga.predict(model=model,\n",
    "                                    solution=solution,\n",
    "                                    data=test_inputs)\n",
    "\n",
    "mae = tensorflow.keras.losses.MeanAbsoluteError()\n",
    "abs_error = mae(data_outputs, predictions).numpy()\n",
    "print(\"Absolute Error : \", abs_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2379280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score :  -0.07967985531976107\n",
      "Testing Score :  -0.060296731196886943\n"
     ]
    }
   ],
   "source": [
    "print('Training Score : ', r2_score(y_train, predictions))\n",
    "print ('Testing Score : ', r2_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d3932",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b8433cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "168/168 [==============================] - 1s 2ms/step - loss: 9.5095 - val_loss: 7.8749\n",
      "Epoch 2/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6.5197 - val_loss: 4.9374\n",
      "Epoch 3/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 3.8327 - val_loss: 2.8305\n",
      "Epoch 4/100\n",
      "168/168 [==============================] - ETA: 0s - loss: 2.576 - 0s 2ms/step - loss: 2.5545 - val_loss: 2.2144\n",
      "Epoch 5/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 2.1658 - val_loss: 1.9647\n",
      "Epoch 6/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.9461 - val_loss: 1.7481\n",
      "Epoch 7/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.7514 - val_loss: 1.5635\n",
      "Epoch 8/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5756 - val_loss: 1.3666\n",
      "Epoch 9/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4155 - val_loss: 1.2105\n",
      "Epoch 10/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.2544 - val_loss: 1.0488\n",
      "Epoch 11/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.1117 - val_loss: 0.9048\n",
      "Epoch 12/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.9795 - val_loss: 0.7754\n",
      "Epoch 13/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.8661 - val_loss: 0.6712\n",
      "Epoch 14/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.7517 - val_loss: 0.5621\n",
      "Epoch 15/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.6553 - val_loss: 0.4864\n",
      "Epoch 16/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.5662 - val_loss: 0.4076\n",
      "Epoch 17/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.4896 - val_loss: 0.3302\n",
      "Epoch 18/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.3998 - val_loss: 0.2520\n",
      "Epoch 19/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.3301 - val_loss: 0.2040\n",
      "Epoch 20/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.2824 - val_loss: 0.1705\n",
      "Epoch 21/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.2427 - val_loss: 0.1429\n",
      "Epoch 22/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.2123 - val_loss: 0.1410\n",
      "Epoch 23/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1915 - val_loss: 0.1178\n",
      "Epoch 24/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1738 - val_loss: 0.1088\n",
      "Epoch 25/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1609 - val_loss: 0.1090\n",
      "Epoch 26/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1559 - val_loss: 0.0999\n",
      "Epoch 27/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1494 - val_loss: 0.1012\n",
      "Epoch 28/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1446 - val_loss: 0.1000\n",
      "Epoch 29/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1409 - val_loss: 0.0992\n",
      "Epoch 30/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1398 - val_loss: 0.1007\n",
      "Epoch 31/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1373 - val_loss: 0.1100\n",
      "Epoch 32/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1346 - val_loss: 0.1171\n",
      "Epoch 33/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1341 - val_loss: 0.1010\n",
      "Epoch 34/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1344 - val_loss: 0.1015\n",
      "Epoch 35/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1334 - val_loss: 0.1103\n",
      "Epoch 36/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1341 - val_loss: 0.1044\n",
      "Epoch 37/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1326 - val_loss: 0.1080\n",
      "Epoch 38/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1291 - val_loss: 0.1057\n",
      "Epoch 39/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1330 - val_loss: 0.1092\n",
      "Epoch 40/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1312 - val_loss: 0.1027\n",
      "Epoch 41/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1306 - val_loss: 0.1087\n",
      "Epoch 42/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1269 - val_loss: 0.1043\n",
      "Epoch 43/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1250 - val_loss: 0.1033\n",
      "Epoch 44/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1297 - val_loss: 0.1025\n",
      "Epoch 45/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1280 - val_loss: 0.1043\n",
      "Epoch 46/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1291 - val_loss: 0.1042\n",
      "Epoch 47/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1294 - val_loss: 0.1094\n",
      "Epoch 48/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1276 - val_loss: 0.1034\n",
      "Epoch 49/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1247 - val_loss: 0.1041\n",
      "Epoch 50/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1277 - val_loss: 0.1027\n",
      "Epoch 51/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1275 - val_loss: 0.1053\n",
      "Epoch 52/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1275 - val_loss: 0.1030\n",
      "Epoch 53/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1264 - val_loss: 0.1022\n",
      "Epoch 54/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1251 - val_loss: 0.1035\n",
      "Epoch 55/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1254 - val_loss: 0.1047\n",
      "Epoch 56/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1260 - val_loss: 0.1027\n",
      "Epoch 57/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1257 - val_loss: 0.1068\n",
      "Epoch 58/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1277 - val_loss: 0.1049\n",
      "Epoch 59/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1297 - val_loss: 0.1027\n",
      "Epoch 60/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1240 - val_loss: 0.1024\n",
      "Epoch 61/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1256 - val_loss: 0.1069\n",
      "Epoch 62/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1237 - val_loss: 0.1021\n",
      "Epoch 63/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1223 - val_loss: 0.1023\n",
      "Epoch 64/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.1028\n",
      "Epoch 65/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1246 - val_loss: 0.1015\n",
      "Epoch 66/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1243 - val_loss: 0.1018\n",
      "Epoch 67/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1222 - val_loss: 0.1016\n",
      "Epoch 68/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1214 - val_loss: 0.1124\n",
      "Epoch 69/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1249 - val_loss: 0.1017\n",
      "Epoch 70/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1247 - val_loss: 0.1042\n",
      "Epoch 71/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1215 - val_loss: 0.1029\n",
      "Epoch 72/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1239 - val_loss: 0.1070\n",
      "Epoch 73/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1218 - val_loss: 0.1071\n",
      "Epoch 74/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1222 - val_loss: 0.1081\n",
      "Epoch 75/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1199 - val_loss: 0.1085\n",
      "Epoch 76/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1220 - val_loss: 0.1023\n",
      "Epoch 77/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.1052\n",
      "Epoch 78/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1208 - val_loss: 0.1036\n",
      "Epoch 79/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1214 - val_loss: 0.1084\n",
      "Epoch 80/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1209 - val_loss: 0.1022\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1185 - val_loss: 0.1013\n",
      "Epoch 82/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1191 - val_loss: 0.1021\n",
      "Epoch 83/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1217 - val_loss: 0.1095\n",
      "Epoch 84/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1220 - val_loss: 0.1170\n",
      "Epoch 85/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1222 - val_loss: 0.1078\n",
      "Epoch 86/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1217 - val_loss: 0.1043\n",
      "Epoch 87/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1189 - val_loss: 0.1011\n",
      "Epoch 88/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1171 - val_loss: 0.1008\n",
      "Epoch 89/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1187 - val_loss: 0.1040\n",
      "Epoch 90/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1159 - val_loss: 0.1011\n",
      "Epoch 91/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1223 - val_loss: 0.1011\n",
      "Epoch 92/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1182 - val_loss: 0.0998\n",
      "Epoch 93/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1175 - val_loss: 0.1052\n",
      "Epoch 94/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1177 - val_loss: 0.1049\n",
      "Epoch 95/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1213 - val_loss: 0.1007\n",
      "Epoch 96/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1187 - val_loss: 0.1004\n",
      "Epoch 97/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1181 - val_loss: 0.1009\n",
      "Epoch 98/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 0.1015\n",
      "Epoch 99/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1179 - val_loss: 0.1056\n",
      "Epoch 100/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.1184 - val_loss: 0.1011\n",
      "Training Result of syn 0.9513557440605254\n",
      "Training Result of syn 0.9152051583663485\n"
     ]
    }
   ],
   "source": [
    "input_layer  = tensorflow.keras.layers.Input(5)\n",
    "dense_layer1 = tensorflow.keras.layers.Dense(5, activation=\"relu\")(input_layer)\n",
    "output = tensorflow.keras.layers.Dense(1, activation=\"linear\")(dense_layer1)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(loss=\"mean_squared_error\" , optimizer=\"adam\")\n",
    "history = model.fit(X_train, y_train, batch_size=2, epochs=100, verbose=1, validation_split=0.2)\n",
    "pred_train = model.predict(X_train)\n",
    "print('Training Result of syn',r2_score(y_train, pred_train))\n",
    "pred_train = model.predict(X_test)\n",
    "print('Training Result of syn',r2_score(y_test, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac572268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
